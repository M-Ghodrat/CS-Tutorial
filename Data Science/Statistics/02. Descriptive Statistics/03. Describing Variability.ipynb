{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "weird-maryland",
   "metadata": {},
   "source": [
    "# Describing Variability\n",
    "\n",
    "Statistics flourishes because we live in a world of variability; no two people are identical, and a few are really far out. When summarizing a set of data, we specify not only measures of central tendency, such as the mean, but also measures of variability, that is, measures of the amount by which scores are dispersed or scattered in a distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-military",
   "metadata": {},
   "source": [
    "<img src=\"../images/variability.png\" alt=\"variability\" width=600 align=\"left\" />\n",
    "\n",
    "Three distributions with **the same mean (10)** but **different amounts of variability**. Numbers\n",
    "in the boxes indicate distances from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-occurrence",
   "metadata": {},
   "source": [
    "<img src=\"../images/variability-experiment.png\" alt=\"variability-experiment\" width=600 align=\"left\" />\n",
    "\n",
    "Two experiments with **the same mean difference** but **dissimilar variabilities**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-stuart",
   "metadata": {},
   "source": [
    "Variabilities within groups assume a key role in inferential statistics. Briefly, the relatively smaller variabilities within groups translate into more **statistical stability**. On the other hand, the relatively larger variabilities within groups, translate into less statistical stability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "future-sculpture",
   "metadata": {},
   "source": [
    "## Range\n",
    "\n",
    "The range is the difference between the largest and smallest scores.\n",
    "\n",
    "The range has **several shortcomings**.\n",
    "\n",
    "- First, since its value depends on only two scores—the largest and the smallest—it fails to use the information provided by the remaining scores.\n",
    "\n",
    "- Furthermore, the value of the range tends to increase with increases in the total number of scores. For instance, the range of adult heights might be 6 or 8 inches for a half a dozen people, whereas it might be 14 or 16 inches for six dozen people. Larger groups are more likely to include very short or very tall people who, of course, inflate the value of the range.\n",
    "\n",
    "**Instead of being a relatively stable measure of variability, the size of the range tends to vary with the size of the group.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-sydney",
   "metadata": {},
   "source": [
    "## Variance\n",
    "\n",
    "Although both the range and its most important spinoff, the interquartile range (discussed later), serve as valid measures of variability, neither is among the statistician’s preferred measures of variability.\n",
    "\n",
    "Those roles are reserved for the variance and particularly for its square root, the standard deviation, because these measures serve as key components for other important statistical measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-cleaner",
   "metadata": {},
   "source": [
    "Following the formula below, we could calculate the value of the variance for each of the three distributions shown earlier.\n",
    "\n",
    "Variance for **Population**:\n",
    "$\\sigma^2 = \\frac{\\sum (x_i - \\mu) ^ 2}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-midnight",
   "metadata": {},
   "source": [
    "Its value equals 0.00 for the least variable distribution, A, 0.29 for the moderately variable distribution, B, and 3.14 for the most variable distribution, C, in agreement with our intuitive judgments about the relative variability of these three distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threaded-trash",
   "metadata": {},
   "source": [
    "**Weakness of Variance**: In the case of a data of the weights of 53 male students, it is useful to know that the mean for the distribution of weights equals 169.51 pounds, but it is confusing to know that, because of the squared deviations, the variance for the same distribution equals 544.29 squared pounds. What, you might reasonably ask, are squared pounds?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-municipality",
   "metadata": {},
   "source": [
    "## Standard Deviation\n",
    "\n",
    "To rid ourselves of these mind-boggling units of measurement, simply take the square root of the variance. This produces a new measure, known as the standard deviation, that describes variability in the original units of measurement.\n",
    "\n",
    "$S = \\sqrt{\\text{Variance}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-engineering",
   "metadata": {},
   "source": [
    "**Note:** You might find it helpful to think of the standard deviation as a rough measure of the average (or standard) amount by which scores deviate on either side of their mean.\n",
    "\n",
    "Strictly speaking, **the standard deviation usually exceeds the mean deviation** or, more accurately, the mean absolute deviation. (In the case of distribution C in the figure on top, for example, the standard deviation equals 1.77, while the mean absolute deviation equals only 1.43.) Nevertheless, it is reasonable to describe the standard deviation as the average amount by which scores deviate on either side of their mean—as long as you **remember that an approximation is involved**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-defensive",
   "metadata": {},
   "source": [
    "#### Majority of Scores within One Standard Deviation\n",
    "\n",
    "A slightly different perspective makes the standard deviation even more accessible. For most frequency distributions, a majority (often as many as 68 percent) of all scores are within one standard deviation on either side of the mean.\n",
    "\n",
    "#### A Small Minority of Scores Deviate More Than Two Standard Deviations\n",
    "For most frequency distributions, a small minority (often as small as 5 percent) of all scores deviate more than two standard deviations on either side of the mean.\n",
    "\n",
    "\n",
    "**Note:** These two generalizations about the majority and minority of scores are independent of the particular shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-evolution",
   "metadata": {},
   "source": [
    "<img src=\"../images/minority-majority.png\" alt=\"minority-majority\" width=300 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-effort",
   "metadata": {},
   "source": [
    "**Note:** The mean is a measure of **position**, but the standard deviation is a measure of **distance** (on either side of the mean of the distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-enhancement",
   "metadata": {},
   "source": [
    "#### Computational Check\n",
    "With rare exceptions, the standard deviation should be less than one-half the size of the range, and in most cases, it will be an even smaller fraction (one-third to one-sixth) the size of the range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "humanitarian-command",
   "metadata": {},
   "source": [
    "## Standard Deviation and Variance for Sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-murray",
   "metadata": {},
   "source": [
    "Although the sum of squares ($SS$) term ($\\sum (x_i - \\mu)^2$)  remains essentially the same for both populations and samples, there is a small but important change in the formulas for the variance and standard deviation for samples. This change appears in the denominator of each formula where $N$, the population size, is replaced not by $n$, the sample size, but by $n − 1$, as shown:\n",
    "\n",
    "Variance for **Sample**: $s^2 = \\frac{SS}{n-1}$\n",
    "\n",
    "Standard Deviation for **Sample**: $s = \\sqrt{s^2} = \\sqrt{\\frac{SS}{n-1}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-feedback",
   "metadata": {},
   "source": [
    "### Why $n-1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rolled-monday",
   "metadata": {},
   "source": [
    "Using $n − 1$ in the denominator of standard deviation and variance solves a problem in inferential statistics associated with generalizations from samples to populations. The adequacy of these generalizations usually depends on accurately estimating unknown variability in the population with known variability in the sample. But if we were to use $n$ rather than $n − 1$ in the denominator of our estimates, they would tend to underestimate variability in the population because $n$ is too large. This tendency would compromise any subsequent generalizations, such as whether observed mean differences are real or merely transitory. On the other hand, when the denominator is made smaller by using $n − 1$, variability in the population is estimated more accurately, and subsequent generalizations are more likely to be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-regular",
   "metadata": {},
   "source": [
    "<img src=\"../images/table-4.3.png\" alt=\"table-4.3\" width=500 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-columbia",
   "metadata": {},
   "source": [
    "Assume that the five scores **(7, 3, 1, 0, 4)** in **Table 4.3** are a random sample from some population whose unknown variability is to be estimated with the sample variability. To understand why $n − 1$ works, let’s look more closely at deviation scores.\n",
    "\n",
    "The definition formula for the sample sum of squares, specifies that each of the five original scores, $X$, be expressed as positive or negative deviations from their sample mean, $X$, of 3. At this point, a subtle mathematical restriction causes a complication.\n",
    " \n",
    "It’s always true, as demonstrated on the left-hand side of Table 4.5, that the sum of all scores, when expressed as deviations about their own mean, equals zero. (If you’re skeptical, recall the discussion that the mean as a balance point that equalizes the sums of all positive and negative deviations.)\n",
    "\n",
    "Given values for any four of the five deviations on the left-hand side of **Table 4.5**, the value of the remaining deviation is not free to vary. Instead, its value is completely fixed because it must comply with the mathematical restriction that the sum of all deviations about their own mean equals zero. For instance, given the sum for the four top deviations on the left-hand side of Table 4.5, that is, `[4 + 0 + (–2) + (–3) = –1]`, the value of the bottom deviation must equal 1, as it does, because of the zero-sum restriction, that is, `[–1 + 1 = 0]`. Or since this mathematical restriction applies to any four of the five deviations, given the sum for the four bottom deviations in Table 4.5, that is, `[0 + (–2) + (–3) + 1 = – 4]`, the value of the top deviation must equal 4 because `[– 4 + 4 = 0]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-surveillance",
   "metadata": {},
   "source": [
    "<img src=\"../images/two-estimate-of-variance.png\" alt=\"two-estimate-of-variance\" width=500 align=\"left\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-conditions",
   "metadata": {},
   "source": [
    "Only $n − 1$ of the sample deviations supply valid information for estimating variability. One bit of valid information has been lost because of the zero-sum restriction when the sample mean replaces the population mean. And that’s why we divide the sum of squares for $X – \\bar{X}$ by $n − 1$, as on the left-hand side of Table 4.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handled-tuition",
   "metadata": {},
   "source": [
    "**Note:** The zero-sum restriction applies\n",
    "only if the five deviations are expressed around their own mean—that is, the sample mean, $X$, of 3. It does not apply when the five deviations are expressed around some\n",
    "other mean, such as the population mean, $\\mu$, of 2 for the entire population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-equipment",
   "metadata": {},
   "source": [
    "## DEGREES OF FREEDOM (df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-christopher",
   "metadata": {},
   "source": [
    "Technically, we have been discussing a very important notion in inferential statistics known as degrees of freedom. Degrees of freedom (df) refers to the number of values that are free to vary, given one or more mathematical restrictions, in a sample being used to estimate a population characteristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-grill",
   "metadata": {},
   "source": [
    "The concept of degrees of freedom is introduced only because we are using scores in a sample to estimate some unknown characteristic of the population. Typically, when used as an estimate, not all observed values in the sample are free to vary because of one or more mathematical restrictions. As has been noted, when n deviations about the sample mean are used to estimate variability in the population, only $n − 1$ are free to vary. As a result, there are only $n − 1$ degrees of freedom, that is, $df = n − 1$. One df is lost because of the zero-sum restriction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-consciousness",
   "metadata": {},
   "source": [
    "The concept of degrees of freedom is introduced only because we are using scores in a sample to estimate some unknown characteristic of the population. Typically, when used as an estimate, not all observed values in the sample are free to vary because of one or more mathematical restrictions. As has been noted, when n deviations about the sample mean are used to estimate variability in the population, only $n − 1$ are free to vary. As a result, there are only $n − 1$ degrees of freedom, that is, $df = n − 1$. One df is lost because of the zero-sum restriction.\n",
    "\n",
    "If the sample sum of squares were divided by $n$, it would tend to underestimate variability in the population. (In Table 4.5, when $\\mu$ is unknown, division by $n$ instead of $n − 1$ would produce a smaller estimate of 6.00 instead of 7.50.) This would occur because there are only $n − 1$ independent deviations (estimates of variability) in the sample sum of squares. A more accurate estimate is obtained when the denominator term reflects the number of independent deviations—that is, the number of degrees of freedom—in the numerator, as in the formulas for $s^2$ and $s$. In fact, we can use degrees of freedom to rewrite the formulas for the sample variance and standard deviation:\n",
    "\n",
    "Variance for **Sample**: $s^2 = \\frac{SS}{n-1} = \\frac{SS}{df}$\n",
    "\n",
    "Standard Deviation for **Sample**: $s = \\sqrt{\\frac{SS}{n-1}} = \\sqrt{\\frac{SS}{df}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-loading",
   "metadata": {},
   "source": [
    "where $s^2$ and $s$ represent the sample variance and standard deviation, $SS$ is the sum of squares as defined earlier, and $df$ is the degrees of freedom and equals $n − 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-article",
   "metadata": {},
   "source": [
    "## Interquartile Range (IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-baptist",
   "metadata": {},
   "source": [
    "The most important spinoff of the range, the interquartile range (IQR), is simply the range for the middle 50 percent of the scores. More specifically, the IQR equals the distance between the third quartile (or 75th percentile) and the first quartile (or 25th percentile), that is, after the highest quarter (or top 25 percent) and the lowest quarter (or bottom 25 percent) have been trimmed from the original set of scores.\n",
    "\n",
    "\n",
    "**Notes:** \n",
    "- Since most distributions are spread more widely in their extremities than their middle, the IQR tends to be less than half the size of the range.\n",
    "\n",
    "- A key property of the IQR is its resistance to the distorting effect of extreme scores, or outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-calibration",
   "metadata": {},
   "source": [
    "> Thus, if you are concerned about possible distortions caused by extreme scores, or outliers, **use the IQR as the measure of variability**, along with the **median** (or second quartile) as **the measure of central tendency**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-processor",
   "metadata": {},
   "source": [
    "## Measurement of Variability for Qualitative and Ranked Data\n",
    "\n",
    "Measures of variability are virtually nonexistent for qualitative or nominal data. It is\n",
    "probably adequate to note merely whether scores are evenly divided among the various\n",
    "classes (maximum variability), unevenly divided among the various classes (intermediate\n",
    "variability), or concentrated mostly in one class (minimum variability)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
